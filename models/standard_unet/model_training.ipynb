{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import shutil\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "imgs_list = os.listdir('data/images')",
   "id": "c6fcb52bdecde763"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example, Marks, Time, Date, Address, Closing, Author, Source, Matrices\n",
    "# Path to your labels folder\n",
    "labels_folder = 'data/labels'\n",
    "\n",
    "# Classes to be removed\n",
    "classes_to_remove = [\n",
    "    'Example', 'Marks', 'Time', 'Date', 'Address',\n",
    "    'Closing', 'Author', 'Source', 'Matrices'\n",
    "]\n",
    "\n",
    "# Create a backup folder for removed files\n",
    "backup_folder = 'removed_labels'\n",
    "if not os.path.exists(backup_folder):\n",
    "    os.makedirs(backup_folder)\n",
    "\n",
    "# This pattern matches the tag format in your files\n",
    "# Example: task-34-annotation-10-by-1-tag-Author-0.png\n",
    "tag_pattern = re.compile(r'task-\\d+-annotation-\\d+-by-\\d+-tag-([A-Za-z\\s-]+)-\\d+\\.png')\n",
    "\n",
    "\n",
    "def should_remove(filename):\n",
    "    \"\"\"Check if the file has a tag that should be removed\"\"\"\n",
    "    match = tag_pattern.match(filename)\n",
    "    if not match:\n",
    "        return False\n",
    "\n",
    "    # Extract the tag name (like \"Author\" or \"Font - Bold\")\n",
    "    tag_name = match.group(1)\n",
    "\n",
    "    for class_name in classes_to_remove:\n",
    "        # Check if tag equals the class or starts with the class\n",
    "        # This handles both \"Author\" and \"Author-0\" cases\n",
    "        if (tag_name == class_name or\n",
    "                tag_name.startswith(class_name + ' ') or\n",
    "                tag_name.startswith(class_name + '-')):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Track statistics\n",
    "total_files = 0\n",
    "removed_files = 0\n",
    "files_kept = 0\n",
    "\n",
    "# Store removed files for reporting\n",
    "removed_file_list = []\n",
    "\n",
    "# Process all files in the labels folder\n",
    "for filename in os.listdir(labels_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        total_files += 1\n",
    "        file_path = os.path.join(labels_folder, filename)\n",
    "\n",
    "        if should_remove(filename):\n",
    "            removed_files += 1\n",
    "            removed_file_list.append(filename)\n",
    "\n",
    "            print(f\"Moving to backup: {filename}\")\n",
    "            shutil.move(file_path, os.path.join(backup_folder, filename))\n",
    "        else:\n",
    "            files_kept += 1\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Total files processed: {total_files}\")\n",
    "print(f\"Files removed: {removed_files}\")\n",
    "print(f\"Files kept: {files_kept}\")\n",
    "\n",
    "if removed_files > 0:\n",
    "    print(\"\\nFiles that were moved to backup:\")\n",
    "    for file in sorted(removed_file_list):\n",
    "        print(f\"  - {file}\")"
   ],
   "id": "d18fe37a3b199dfd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Loader",
   "id": "cf768192ed90ca3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DocumentSegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None, target_size=(512, 512)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Path to the directory with document images\n",
    "            labels_dir (str): Path to the directory with label mask images\n",
    "            transform (callable, optional): Optional transform to be applied on images\n",
    "            target_size (tuple): Size to resize images and masks to\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        # Get all document image files\n",
    "        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "\n",
    "        # Extract document IDs from image filenames (e.g., \"doc_31.jpg\" -> \"31\")\n",
    "        self.doc_ids = []\n",
    "        for img_file in self.image_files:\n",
    "            match = re.search(r'doc_(\\d+)', img_file)\n",
    "            if match:\n",
    "                self.doc_ids.append(match.group(1))\n",
    "\n",
    "        # Define class names and assign index to each class\n",
    "        self.class_names = [\n",
    "            'Background',  # 0\n",
    "            'Header',  # 1\n",
    "            'Paragraph',  # 2\n",
    "            'Page Number',  # 3\n",
    "            'Footnotes',  # 4\n",
    "            'Font - Bold',  # 5\n",
    "            'Topic',  # 6\n",
    "            'Caption',  # 7\n",
    "            'Image',  # 8\n",
    "            'Topic - Level1',  # 9\n",
    "            'Topic - Level2',  # 10\n",
    "            'Topic - Level3',  # 11\n",
    "            'Diagram',  # 12\n",
    "            'Table',  # 13\n",
    "            'Mathematical Expression',  # 14\n",
    "            'List',  # 15\n",
    "            'Footer',  # 16\n",
    "            'Font - Italic',  # 17\n",
    "            'Poem',  # 18\n",
    "            'Charts',  # 19\n",
    "        ]\n",
    "\n",
    "        # Exclude the classes we want to remove\n",
    "        self.excluded_classes = [\n",
    "            'Example', 'Marks', 'Time', 'Date', 'Address',\n",
    "            'Closing', 'Author', 'Source', 'Matrices'\n",
    "        ]\n",
    "\n",
    "        # Create a mapping from class name to index\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.class_names)}\n",
    "        self.num_classes = len(self.class_names)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load document image\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize image to target size\n",
    "        image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Get corresponding document ID\n",
    "        doc_id = self.doc_ids[idx]\n",
    "\n",
    "        # Create empty segmentation mask (initialized with zeros for background)\n",
    "        mask = np.zeros((*self.target_size, 1), dtype=np.uint8)\n",
    "\n",
    "        # Find all label masks for this document\n",
    "        label_pattern = f\"task-*-annotation-*-by-*-tag-*-*.png\"\n",
    "        all_labels = os.listdir(self.labels_dir)\n",
    "        doc_labels = []\n",
    "\n",
    "        # Find the annotation number for this document (assuming consistent annotation numbers)\n",
    "        annotation_numbers = set()\n",
    "        for label_file in all_labels:\n",
    "            # Look for any label files related to this document\n",
    "            if f\"-{doc_id}-\" in label_file:\n",
    "                match = re.search(r'task-(\\d+)-annotation', label_file)\n",
    "                if match:\n",
    "                    annotation_numbers.add(match.group(1))\n",
    "\n",
    "        # If we found annotation numbers, use them to find all related labels\n",
    "        if annotation_numbers:\n",
    "            for annotation_num in annotation_numbers:\n",
    "                # Get all masks for this annotation\n",
    "                doc_labels.extend([f for f in all_labels if f.startswith(f\"task-{annotation_num}-annotation\")])\n",
    "\n",
    "        # Process each label mask\n",
    "        for label_file in doc_labels:\n",
    "            # Extract class name from label file (e.g., \"task-31-annotation-4-by-1-tag-Header-0.png\" -> \"Header\")\n",
    "            match = re.search(r'tag-([A-Za-z\\s-]+)-\\d+', label_file)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            class_name = match.group(1)\n",
    "\n",
    "            # Skip excluded classes\n",
    "            if any(excluded in class_name for excluded in self.excluded_classes):\n",
    "                continue\n",
    "\n",
    "            # Check if this class is in our mapping\n",
    "            if class_name not in self.class_to_idx:\n",
    "                # Add new class if not already present\n",
    "                self.class_names.append(class_name)\n",
    "                self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                self.num_classes = len(self.class_names)\n",
    "\n",
    "            # Get class index\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "            # Load label mask\n",
    "            label_path = os.path.join(self.labels_dir, label_file)\n",
    "            label_mask = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            if label_mask is None:\n",
    "                continue\n",
    "\n",
    "            # Resize label mask to match target size\n",
    "            label_mask = cv2.resize(label_mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            # Binary threshold to ensure mask is binary (0 or 255)\n",
    "            _, label_mask = cv2.threshold(label_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Set corresponding pixels in the combined mask to class index\n",
    "            # For pixels where the label mask is > 0, set the class value\n",
    "            mask[label_mask > 0] = class_idx\n",
    "\n",
    "        # Convert image and mask to tensors\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        # Apply additional transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask.squeeze(),\n",
    "            'doc_id': doc_id,\n",
    "            'image_path': img_path\n",
    "        }\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return self.class_names"
   ],
   "id": "12c6ee451e4e9a5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_sample(dataset, idx):\n",
    "    \"\"\"Visualize a sample from the dataset\"\"\"\n",
    "    sample = dataset[idx]\n",
    "    image = sample['image']\n",
    "    mask = sample['mask']\n",
    "    doc_id = sample['doc_id']\n",
    "\n",
    "    # Convert tensor to numpy for visualization\n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    mask_np = mask.numpy()\n",
    "\n",
    "    # Create a color-coded mask for visualization\n",
    "    cmap = plt.cm.get_cmap('tab20', dataset.num_classes)\n",
    "    colored_mask = cmap(mask_np)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(f\"Document {doc_id}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(colored_mask)\n",
    "    plt.title(f\"Segmentation Mask\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Add color bar with class names\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(0, dataset.num_classes - 1))\n",
    "    cbar = plt.colorbar(sm, ax=plt.gca())\n",
    "    cbar.set_ticks(np.arange(dataset.num_classes) + 0.5)\n",
    "    cbar.set_ticklabels(dataset.class_names)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "aad11d9534cef34d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define directories\n",
    "images_dir = 'data/images'  # Update with your actual path\n",
    "labels_dir = 'data/labels'  # Update with your actual path\n",
    "\n",
    "# Create dataset\n",
    "dataset = DocumentSegmentationDataset(images_dir, labels_dir, target_size=(256, 256))\n",
    "\n",
    "# Create dataloader\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Class names: {dataset.class_names}\")\n",
    "\n",
    "# Visualize a sample\n",
    "visualize_sample(dataset, 0)\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    images = batch['image']\n",
    "    masks = batch['mask']\n",
    "    doc_ids = batch['doc_id']\n",
    "\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  Image shape: {images.shape}\")\n",
    "    print(f\"  Mask shape: {masks.shape}\")\n",
    "    print(f\"  Document IDs: {doc_ids}\")\n",
    "\n",
    "    # Process only one batch for demonstration\n",
    "    if batch_idx == 0:\n",
    "        break"
   ],
   "id": "6f4d1e64d4f7bcb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## U-Net Components",
   "id": "2fbcdba5cdd65f8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv => BN => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ],
   "id": "25b2d77e8df57e86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        # Encoder path\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "\n",
    "        # Final output layer\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        # Output layer\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ],
   "id": "f463332d26fc8f3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Loop",
   "id": "455205fccfb35980"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=50, lr=0.001):\n",
    "    # Criterion (loss function) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    # Track losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Create directory for model checkpoints\n",
    "    checkpoint_dir = os.path.join('checkpoints', datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        with tqdm(train_loader, unit=\"batch\", desc=\"Training\") as train_pbar:\n",
    "            for batch in train_pbar:\n",
    "                images = batch['image'].to(device)\n",
    "                masks = batch['mask'].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                train_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase - SIMPLIFIED\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        # Add tqdm progress bar for validation\n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, unit=\"batch\", desc=\"Validation\") as val_pbar:\n",
    "                for batch in val_pbar:\n",
    "                    images = batch['image'].to(device)\n",
    "                    masks = batch['mask'].to(device)\n",
    "\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Update progress bar with current loss\n",
    "                    batch_loss = loss.item()\n",
    "                    val_pbar.set_postfix(loss=batch_loss)\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "            print(f\"✅ Saved best model with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth'))\n",
    "            print(f\"💾 Saved checkpoint at epoch {epoch + 1}\")\n",
    "\n",
    "        # Print summary of losses only\n",
    "        print(f'Summary - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Plot only training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'loss_plot.png'))\n",
    "    plt.show()\n",
    "\n",
    "    return model, {\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses\n",
    "    }"
   ],
   "id": "ad50c2b8c52f13ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_predictions(model, dataset, indices, device, save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize model predictions for multiple samples.\n",
    "\n",
    "    Args:\n",
    "        model: The trained UNet model\n",
    "        dataset: The dataset containing images and masks\n",
    "        indices: List of sample indices to visualize\n",
    "        device: The device to run inference on\n",
    "        save_dir: Directory to save visualizations (optional)\n",
    "    \"\"\"\n",
    "    if save_dir and not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    model.eval()\n",
    "    class_names = dataset.get_class_names()\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    # Create a colormap\n",
    "    cmap = plt.cm.get_cmap('tab20', n_classes)\n",
    "\n",
    "    for idx in indices:\n",
    "        sample = dataset[idx]\n",
    "        image = sample['image'].unsqueeze(0).to(device)\n",
    "        true_mask = sample['mask'].numpy()\n",
    "        doc_id = sample['doc_id']\n",
    "\n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "        # Convert image tensor to numpy\n",
    "        image_np = image.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Create figure with subplots\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "        # Original Image\n",
    "        ax[0, 0].imshow(image_np)\n",
    "        ax[0, 0].set_title(f\"Document {doc_id}\")\n",
    "        ax[0, 0].axis('off')\n",
    "\n",
    "        # Ground Truth Mask\n",
    "        ax[0, 1].imshow(cmap(true_mask))\n",
    "        ax[0, 1].set_title(\"Ground Truth\")\n",
    "        ax[0, 1].axis('off')\n",
    "\n",
    "        # Prediction Mask\n",
    "        ax[1, 0].imshow(cmap(pred_mask))\n",
    "        ax[1, 0].set_title(\"Prediction\")\n",
    "        ax[1, 0].axis('off')\n",
    "\n",
    "        # Difference Mask\n",
    "        diff_mask = (pred_mask != true_mask).astype(np.int32)\n",
    "        ax[1, 1].imshow(image_np)\n",
    "        ax[1, 1].imshow(np.ma.masked_where(diff_mask == 0, diff_mask),\n",
    "                        alpha=0.7, cmap='cool')\n",
    "        ax[1, 1].set_title(\"Errors (Misclassified Pixels)\")\n",
    "        ax[1, 1].axis('off')\n",
    "\n",
    "        # Add colorbar\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(0, n_classes - 1))\n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "        cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "        cbar.set_ticks(np.arange(n_classes) + 0.5)\n",
    "        cbar.set_ticklabels(class_names)\n",
    "\n",
    "        # Calculate per-class metrics for this sample\n",
    "        class_metrics = {}\n",
    "        for c in range(n_classes):\n",
    "            # Calculate metrics for this class\n",
    "            true_c = (true_mask == c)\n",
    "            pred_c = (pred_mask == c)\n",
    "\n",
    "            if np.sum(true_c) == 0:\n",
    "                # Skip if class not present in ground truth\n",
    "                continue\n",
    "\n",
    "            intersection = np.logical_and(true_c, pred_c)\n",
    "            union = np.logical_or(true_c, pred_c)\n",
    "\n",
    "            tp = np.sum(intersection)\n",
    "            fp = np.sum(pred_c) - tp\n",
    "            fn = np.sum(true_c) - tp\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            iou = tp / np.sum(union) if np.sum(union) > 0 else 0\n",
    "\n",
    "            class_metrics[class_names[c]] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'iou': iou,\n",
    "                'pixels': np.sum(true_c)\n",
    "            }\n",
    "\n",
    "        # Add a text box with metrics\n",
    "        metrics_text = \"Per-class metrics:\\n\"\n",
    "        for cls, metrics in class_metrics.items():\n",
    "            metrics_text += f\"{cls}: IoU={metrics['iou']:.2f}, F1={metrics['f1']:.2f}\\n\"\n",
    "\n",
    "        # Add text box with metrics\n",
    "        props = dict(boxstyle='round', alpha=0.5)\n",
    "        fig.text(0.02, 0.02, metrics_text, fontsize=10,\n",
    "                 verticalalignment='bottom', bbox=props)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "        plt.suptitle(f\"Document Segmentation Analysis (Sample {idx})\", fontsize=16, y=0.98)\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, f\"pred_{idx}.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n"
   ],
   "id": "7d8724cfad0f3732"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_class_distribution(dataset):\n",
    "    \"\"\"\n",
    "    Analyze the class distribution in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset to analyze\n",
    "    \"\"\"\n",
    "    class_names = dataset.get_class_names()\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    # Count pixels per class\n",
    "    class_counts = np.zeros(n_classes)\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc=\"Analyzing classes\"):\n",
    "        mask = dataset[i]['mask'].numpy()\n",
    "        for c in range(n_classes):\n",
    "            class_counts[c] += np.sum(mask == c)\n",
    "\n",
    "    # Plot class distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(class_names, class_counts)\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Pixel Count')\n",
    "    plt.title('Class Distribution in Dataset')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Add counts on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2., height + 0.1,\n",
    "                 f'{int(height)}', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate class weights for potential weighted loss\n",
    "    total_pixels = np.sum(class_counts)\n",
    "    class_weights = 1.0 / (class_counts + 1e-10)  # Add small epsilon to avoid division by zero\n",
    "    class_weights = class_weights / np.sum(class_weights) * n_classes  # Normalize\n",
    "\n",
    "    print(\"\\nClass weights for weighted loss:\")\n",
    "    for i, (name, weight) in enumerate(zip(class_names, class_weights)):\n",
    "        print(f\"{name}: {weight:.4f}\")\n",
    "\n",
    "    return class_counts, class_weights"
   ],
   "id": "ee32da183ebd7f70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_training(images_dir, labels_dir, target_size=(512, 512), batch_size=4, epochs=50):\n",
    "    # Create dataset and split to train/val\n",
    "    full_dataset = DocumentSegmentationDataset(images_dir, labels_dir, target_size=target_size)\n",
    "\n",
    "    # Define train/val split ratio\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "\n",
    "    # Split dataset\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size])\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create model\n",
    "    model = UNet(n_channels=3, n_classes=full_dataset.num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Print model summary\n",
    "    print(f\"Model: UNet\")\n",
    "    print(f\"Input channels: 3\")\n",
    "    print(f\"Output classes: {full_dataset.num_classes}\")\n",
    "    print(f\"Class names: {full_dataset.get_class_names()}\")\n",
    "\n",
    "    # Train model\n",
    "    trained_model, _ = train_model(\n",
    "        model, train_loader, val_loader, device, epochs=epochs)\n",
    "\n",
    "    # Visualize some predictions\n",
    "    # for i in range(3):  # Show 3 random examples\n",
    "    #     idx = np.random.randint(0, len(val_dataset))\n",
    "    #     visualize_predictions(trained_model, full_dataset, idx, device)\n",
    "\n",
    "    return trained_model"
   ],
   "id": "9125996864a48531"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trained_model = run_training('data/images', 'data/labels', target_size=(256, 256), batch_size=12, epochs=10)",
   "id": "52ffa13059804da0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = UNet(n_channels=3, n_classes=dataset.num_classes)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "checkpoint_path = 'checkpoints/20250316_221703/best_model.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ],
   "id": "e8b447174207971f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "full_dataset = DocumentSegmentationDataset('data/images', 'data/labels', target_size=(256, 256))\n",
    "visualize_predictions(model, full_dataset, [0, 1, 2], 'cpu')"
   ],
   "id": "44d68db07efc260c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(model, test_loader, device, save_dir=None):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and generate detailed metrics.\n",
    "\n",
    "    Args:\n",
    "        model: The trained UNet model\n",
    "        test_loader: DataLoader for test data\n",
    "        device: Device to run evaluation on\n",
    "        save_dir: Directory to save results (optional)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n_classes = model.n_classes\n",
    "\n",
    "    # Initialize confusion matrix\n",
    "    confusion_matrix = torch.zeros((n_classes, n_classes), device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Update confusion matrix\n",
    "            for t, p in zip(masks.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    # Calculate per-class metrics\n",
    "    precision = torch.zeros(n_classes, device=device)\n",
    "    recall = torch.zeros(n_classes, device=device)\n",
    "    f1 = torch.zeros(n_classes, device=device)\n",
    "    iou = torch.zeros(n_classes, device=device)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        # True Positives: diagonal elements\n",
    "        tp = confusion_matrix[i, i]\n",
    "        # False Positives: sum of column i - true positives\n",
    "        fp = confusion_matrix[:, i].sum() - tp\n",
    "        # False Negatives: sum of row i - true positives\n",
    "        fn = confusion_matrix[i, :].sum() - tp\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision[i] = tp / (tp + fp + 1e-10)\n",
    "\n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall[i] = tp / (tp + fn + 1e-10)\n",
    "\n",
    "        # F1 Score: 2 * (precision * recall) / (precision + recall)\n",
    "        f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i] + 1e-10)\n",
    "\n",
    "        # IoU / Jaccard Index: TP / (TP + FP + FN)\n",
    "        iou[i] = tp / (tp + fp + fn + 1e-10)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    mean_precision = precision[1:].mean() if n_classes > 1 else precision.mean()\n",
    "    mean_recall = recall[1:].mean() if n_classes > 1 else recall.mean()\n",
    "    mean_f1 = f1[1:].mean() if n_classes > 1 else f1.mean()\n",
    "    mean_iou = iou[1:].mean() if n_classes > 1 else iou.mean()\n",
    "\n",
    "    # Calculate pixel accuracy\n",
    "    pixel_accuracy = torch.diag(confusion_matrix).sum() / confusion_matrix.sum()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Pixel Accuracy: {pixel_accuracy:.4f}\")\n",
    "    print(f\"  Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"  Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"  Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"  Mean F1 Score: {mean_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-class Metrics:\")\n",
    "    class_names = test_loader.dataset.get_class_names() if hasattr(test_loader.dataset, 'get_class_names') else [f\"Class {i}\" for i in range(n_classes)]\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        print(f\"  {class_names[i]}:\")\n",
    "        print(f\"    IoU: {iou[i]:.4f}\")\n",
    "        print(f\"    Precision: {precision[i]:.4f}\")\n",
    "        print(f\"    Recall: {recall[i]:.4f}\")\n",
    "        print(f\"    F1 Score: {f1[i]:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    confusion_normalized = confusion_matrix.cpu().numpy()\n",
    "    confusion_normalized = confusion_normalized / (confusion_normalized.sum(axis=1, keepdims=True) + 1e-10)\n",
    "\n",
    "    plt.imshow(confusion_normalized, cmap='Blues')\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Add class names to axes\n",
    "    plt.xticks(np.arange(n_classes), class_names, rotation=45, ha='right')\n",
    "    plt.yticks(np.arange(n_classes), class_names)\n",
    "\n",
    "    # Add text annotations\n",
    "    thresh = confusion_normalized.max() / 2.0\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            plt.text(j, i, f\"{confusion_normalized[i, j]:.2f}\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if confusion_normalized[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title('Confusion Matrix (Normalized)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, \"confusion_matrix.png\"))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Prepare results to return\n",
    "    results = {\n",
    "        'precision': precision.cpu().numpy(),\n",
    "        'recall': recall.cpu().numpy(),\n",
    "        'f1': f1.cpu().numpy(),\n",
    "        'iou': iou.cpu().numpy(),\n",
    "        'pixel_accuracy': pixel_accuracy.item(),\n",
    "        'mean_precision': mean_precision.item(),\n",
    "        'mean_recall': mean_recall.item(),\n",
    "        'mean_f1': mean_f1.item(),\n",
    "        'mean_iou': mean_iou.item(),\n",
    "        'confusion_matrix': confusion_matrix.cpu().numpy()\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "id": "2cca24a6cfdab457"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
